import os
import json
import logging
from typing import List, Dict, Optional, Tuple
import openai
import anthropic
import google.generativeai as genai
from datetime import datetime
import re
import requests
from google.cloud import aiplatform
from google.cloud import storage
import vertexai
from vertexai.language_models import TextGenerationModel
from vertexai.vision_models import ImageGenerationModel

logger = logging.getLogger(__name__)

class HybridAIModel:
    def __init__(self, recipe_manager=None, language_mapper=None, rag_manager=None):
        """í•˜ì´ë¸Œë¦¬ë“œ AI ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.recipe_manager = recipe_manager
        self.language_mapper = language_mapper
        self.rag_manager = rag_manager
        
        # GCP ì„¤ì • (ê¸°ë³¸ í™œì„±í™”)
        self.gcp_project_id = os.getenv('GCP_PROJECT_ID')
        self.gcs_bucket_name = os.getenv('GCS_BUCKET_NAME')
        
        if not self.gcp_project_id or not self.gcs_bucket_name:
            logger.warning("GCP ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
        
        # AI ëª¨ë¸ ì„¤ì •
        self.available_models = self._get_available_models()
        self.current_model = os.getenv('DEFAULT_AI_MODEL', 'gemini-pro')  # Gemini Proë¥¼ ê¸°ë³¸ ëª¨ë¸ë¡œ ì„¤ì •
        
        # AI í´ë¼ì´ì–¸íŠ¸ë“¤ ì´ˆê¸°í™”
        self.clients = self._init_ai_clients()
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.system_prompt = self._load_system_prompt()
        
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self.available_models.keys())}")
    
    def _get_available_models(self) -> Dict:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."""
        return {
            'gemini-pro': {
                'name': 'Gemini Pro (ì›¹ê²€ìƒ‰)',
                'provider': 'google',
                'free_tier': True,
                'web_search': True,  # ì›¹ê²€ìƒ‰ í•­ìƒ í™œì„±í™”
                'max_tokens': 8192,
                'cost_per_1k_tokens': 0.0005,
                'description': 'Googleì˜ ìµœì‹  AI ëª¨ë¸ (ì›¹ê²€ìƒ‰ í¬í•¨, GCP í¬ë ˆë”§ ì‚¬ìš©)',
                'is_main': True  # ë©”ì¸ ëª¨ë¸ë¡œ í‘œì‹œ
            },
            'gpt-3.5-turbo': {
                'name': 'GPT-3.5 Turbo (ë¬´ë£Œ)',
                'provider': 'openai',
                'free_tier': True,
                'web_search': False,  # ì›¹ê²€ìƒ‰ ë¹„í™œì„±í™”
                'max_tokens': 4096,
                'cost_per_1k_tokens': 0.002,
                'description': 'ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ OpenAI ëª¨ë¸ (ë¬´ë£Œ í‹°ì–´)',
                'is_main': False
            },
            'claude-3-haiku': {
                'name': 'Claude 3 Haiku (ë¬´ë£Œ)',
                'provider': 'anthropic',
                'free_tier': True,
                'web_search': False,  # ì›¹ê²€ìƒ‰ ë¹„í™œì„±í™”
                'max_tokens': 4096,
                'cost_per_1k_tokens': 0.00025,
                'description': 'ë¹ ë¥´ê³  ê²½ì œì ì¸ Anthropic ëª¨ë¸ (ë¬´ë£Œ í‹°ì–´)',
                'is_main': False
            },
            # ìœ ë£Œ ëª¨ë¸ë“¤ (ë‚˜ì¤‘ì— í™œì„±í™” ê°€ëŠ¥)
            'gpt-4': {
                'name': 'GPT-4 (ìœ ë£Œ)',
                'provider': 'openai',
                'free_tier': False,
                'web_search': True,
                'max_tokens': 8192,
                'cost_per_1k_tokens': 0.03,
                'description': 'ê³ ì„±ëŠ¥ OpenAI ëª¨ë¸ (ìœ ë£Œ)',
                'is_main': False,
                'enabled': False  # ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”
            },
            'claude-3-sonnet': {
                'name': 'Claude 3 Sonnet (ìœ ë£Œ)',
                'provider': 'anthropic',
                'free_tier': False,
                'web_search': True,
                'max_tokens': 4096,
                'cost_per_1k_tokens': 0.003,
                'description': 'ê· í˜•ì¡íŒ ì„±ëŠ¥ì˜ Anthropic ëª¨ë¸ (ìœ ë£Œ)',
                'is_main': False,
                'enabled': False  # ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”
            }
        }
    
    def _init_ai_clients(self) -> Dict:
        """AI í´ë¼ì´ì–¸íŠ¸ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        clients = {}
        
        try:
            # OpenAI í´ë¼ì´ì–¸íŠ¸
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if openai_api_key:
                clients['openai'] = openai.OpenAI(api_key=openai_api_key)
                logger.info("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        try:
            # Anthropic í´ë¼ì´ì–¸íŠ¸
            anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
            if anthropic_api_key:
                clients['anthropic'] = anthropic.Anthropic(api_key=anthropic_api_key)
                logger.info("Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            logger.error(f"Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        try:
            # Google Gemini í´ë¼ì´ì–¸íŠ¸
            google_api_key = os.getenv('GOOGLE_API_KEY')
            if google_api_key:
                genai.configure(api_key=google_api_key)
                clients['google'] = genai
                logger.info("Google Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            logger.error(f"Google Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        return clients
    
    def get_available_models_info(self) -> List[Dict]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        models_info = []
        
        for model_id, model_info in self.available_models.items():
            provider = model_info['provider']
            is_available = provider in self.clients
            
            models_info.append({
                'id': model_id,
                'name': model_info['name'],
                'provider': provider,
                'free_tier': model_info['free_tier'],
                'description': model_info['description'],
                'available': is_available,
                'current': model_id == self.current_model
            })
        
        return models_info
    
    def switch_model(self, model_id: str) -> bool:
        """AI ëª¨ë¸ì„ ì „í™˜í•©ë‹ˆë‹¤."""
        if model_id not in self.available_models:
            logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_id}")
            return False
        
        provider = self.available_models[model_id]['provider']
        if provider not in self.clients:
            logger.error(f"ëª¨ë¸ {model_id}ì˜ í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        self.current_model = model_id
        logger.info(f"AI ëª¨ë¸ì„ {model_id}ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.")
        return True
    
    def _load_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        base_prompt = """ë‹¹ì‹ ì€ ë§ˆì¸í¬ë˜í”„íŠ¸ ëª¨ë“œíŒ© ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì£¼ìš” ì—­í• :
1. ëª¨ë“œíŒ© ê´€ë ¨ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ ì œê³µ
2. ì•„ì´í…œ ì œì‘ë²•, ì‚¬ìš©ë²•, íšë“ ë°©ë²• ë“± ìƒì„¸ ì •ë³´ ì œê³µ
3. ëª¨ë“œíŒ© ê°„ì˜ ìƒí˜¸ì‘ìš©ê³¼ ìµœì í™” ë°©ë²• ì•ˆë‚´
4. í”Œë ˆì´ì–´ì˜ ì§„í–‰ ìƒí™©ì— ë§ëŠ” ì¡°ì–¸ ì œê³µ

ë‹µë³€ ê·œì¹™:
- í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€
- êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ
- ê²Œì„ ë‚´ ì•„ì´í…œëª…ì€ ì •í™•íˆ í‘œê¸°
- ë³µì¡í•œ ë‚´ìš©ì€ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…
- ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ ìœ ì§€

í˜„ì¬ ëª¨ë“œíŒ©: {modpack_name}
ëª¨ë“œíŒ© ë²„ì „: {modpack_version}

{context_info}

{rag_context}

{web_search_context}

ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
        
        return base_prompt
    
    def _get_local_context(self, message: str, modpack_name: str, modpack_version: str = "1.0", user_uuid: str = None) -> str:
        """ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        if not self.recipe_manager:
            return ""
        
        context_parts = []
        translation_info = []
        
        # ì•„ì´í…œëª… ì¶”ì¶œ ë° ë³€í™˜
        item_names = self._extract_item_names(message)
        
        for item_name in item_names:
            if re.search(r'[ê°€-í£]', item_name):
                english_name, confidence, source = self._translate_korean_to_english(
                    item_name, modpack_name, user_uuid
                )
                
                if english_name:
                    translation_info.append(f"'{item_name}' â†’ '{english_name}' (ì‹ ë¢°ë„: {confidence:.1f})")
                    search_name = english_name
                else:
                    search_name = item_name
                    if confidence < 0.3:
                        context_parts.append(f"âš ï¸ '{item_name}'ì˜ ì˜ì–´ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue
            else:
                search_name = item_name
            
            # ë¡œì»¬ ì œì‘ë²• ì •ë³´
            recipe = self.recipe_manager.get_recipe_with_version_fallback(
                search_name, modpack_name, modpack_version
            )
            if recipe:
                context_parts.append(f"ë¡œì»¬ ì œì‘ë²• - {search_name}: {json.dumps(recipe, ensure_ascii=False)}")
            
            # ë¡œì»¬ ì•„ì´í…œ ì •ë³´
            item_info = self.recipe_manager.get_item_info_with_version_fallback(
                search_name, modpack_name, modpack_version
            )
            if item_info:
                context_parts.append(f"ë¡œì»¬ ì•„ì´í…œ ì •ë³´ - {search_name}: {json.dumps(item_info, ensure_ascii=False)}")
        
        # ë²ˆì—­ ì •ë³´ ì¶”ê°€
        if translation_info:
            context_parts.append(f"ë²ˆì—­ ì •ë³´: {'; '.join(translation_info)}")
        
        return "\n".join(context_parts) if context_parts else ""
    
    def _get_rag_context(self, message: str, modpack_name: str) -> str:
        """GCP RAGì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (í•„ìˆ˜)"""
        if not self.rag_manager:
            logger.error("âŒ RAG ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            raise RuntimeError("RAG ë§¤ë‹ˆì €ê°€ í•„ìˆ˜ì´ì§€ë§Œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            # RAG ë§¤ë‹ˆì €ë¥¼ í†µí•œ ê²€ìƒ‰
            relevant_docs = self.rag_manager.search_similar_documents(message, modpack_name, top_k=3)
            
            if relevant_docs:
                logger.info(f"âœ… RAGì—ì„œ {len(relevant_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
                return f"RAG ì •ë³´:\n{json.dumps(relevant_docs, ensure_ascii=False)}"
            else:
                logger.warning(f"âš ï¸ RAGì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {message}")
                return "RAG ì •ë³´: ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            logger.error(f"âŒ RAG ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            raise RuntimeError(f"RAG ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    def _get_web_search_context(self, message: str, modpack_name: str) -> str:
        """AI ì›¹ê²€ìƒ‰ì„ í†µí•´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            # AIì—ê²Œ ì›¹ê²€ìƒ‰ ìš”ì²­
            search_prompt = f"""
ë‹¤ìŒ ë§ˆì¸í¬ë˜í”„íŠ¸ ëª¨ë“œíŒ© ê´€ë ¨ ì •ë³´ë¥¼ ì›¹ì—ì„œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”:

ëª¨ë“œíŒ©: {modpack_name}
ì§ˆë¬¸: {message}

ê²€ìƒ‰í•´ì•¼ í•  í‚¤ì›Œë“œ:
1. "{modpack_name}" ë§ˆì¸í¬ë˜í”„íŠ¸ ëª¨ë“œíŒ©
2. ê´€ë ¨ ì•„ì´í…œëª…ì´ë‚˜ ëª¨ë“œëª…
3. ì œì‘ë²•, ì‚¬ìš©ë²•, ê°€ì´ë“œ

ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ì´íŠ¸ì—ì„œë§Œ ê²€ìƒ‰í•˜ê³ , ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì„œ ì œê³µí•´ì£¼ì„¸ìš”.
"""
            
            # í˜„ì¬ ëª¨ë¸ì˜ ì œê³µì í™•ì¸
            provider = self.available_models[self.current_model]['provider']
            
            if provider == 'openai':
                return self._openai_web_search(search_prompt)
            elif provider == 'anthropic':
                return self._anthropic_web_search(search_prompt)
            elif provider == 'google':
                return self._google_web_search(search_prompt)
            else:
                return ""
            
        except Exception as e:
            logger.error(f"ì›¹ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return ""
    
    def _openai_web_search(self, search_prompt: str) -> str:
        """OpenAIë¥¼ ì‚¬ìš©í•œ ì›¹ê²€ìƒ‰."""
        try:
            response = self.clients['openai'].chat.completions.create(
                model=self.current_model,
                messages=[{"role": "user", "content": search_prompt}],
                max_tokens=500,
                temperature=0.3
            )
            return f"ì›¹ê²€ìƒ‰ ê²°ê³¼:\n{response.choices[0].message.content.strip()}"
        except Exception as e:
            logger.error(f"OpenAI ì›¹ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return ""
    
    def _anthropic_web_search(self, search_prompt: str) -> str:
        """Anthropicì„ ì‚¬ìš©í•œ ì›¹ê²€ìƒ‰."""
        try:
            response = self.clients['anthropic'].messages.create(
                model=self.current_model,
                max_tokens=500,
                temperature=0.3,
                messages=[{"role": "user", "content": search_prompt}]
            )
            return f"ì›¹ê²€ìƒ‰ ê²°ê³¼:\n{response.content[0].text.strip()}"
        except Exception as e:
            logger.error(f"Anthropic ì›¹ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return ""
    
    def _google_web_search(self, search_prompt: str) -> str:
        """Google Geminië¥¼ ì‚¬ìš©í•œ ì›¹ê²€ìƒ‰."""
        try:
            # Gemini Pro with web search
            model = self.clients['google'].GenerativeModel('gemini-pro')
            
            # ì›¹ê²€ìƒ‰ì„ ìœ„í•œ íŠ¹ë³„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            web_search_prompt = f"""
ë‹¤ìŒ ë§ˆì¸í¬ë˜í”„íŠ¸ ëª¨ë“œíŒ© ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì›¹ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:

{search_prompt}

ê²€ìƒ‰í•  ë•Œ ë‹¤ìŒ ì‚¬ì´íŠ¸ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•´ì£¼ì„¸ìš”:
- CurseForge (ëª¨ë“œíŒ© ì •ë³´)
- MinecraftWiki (ê¸°ë³¸ ì •ë³´)
- Reddit r/feedthebeast (ì»¤ë®¤ë‹ˆí‹°)
- ê³µì‹ ëª¨ë“œ ë¬¸ì„œ

ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
"""
            
            response = model.generate_content(
                web_search_prompt,
                generation_config={
                    'temperature': 0.3,
                    'top_p': 0.8,
                    'top_k': 40,
                    'max_output_tokens': 800,
                }
            )
            
            if response.text:
                logger.info("âœ… Gemini Pro ì›¹ê²€ìƒ‰ ì™„ë£Œ")
                return f"ì›¹ê²€ìƒ‰ ê²°ê³¼ (Gemini Pro):\n{response.text.strip()}"
            else:
                logger.warning("âš ï¸ Gemini Pro ì›¹ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return ""
                
        except Exception as e:
            logger.error(f"âŒ Google ì›¹ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return ""
    
    def _translate_korean_to_english(self, korean_name: str, modpack_name: str, user_uuid: str = None) -> Tuple[Optional[str], float, str]:
        """í•œê¸€ ì•„ì´í…œëª…ì„ ì˜ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not self.language_mapper:
            return None, 0.0, "no_mapper"
        
        english_name, confidence, source = self.language_mapper.find_english_name_hybrid(
            korean_name, modpack_name, user_uuid, self
        )
        
        if english_name and confidence > 0.5:
            self.language_mapper.update_usage_count(korean_name, english_name)
            return english_name, confidence, source
        
        return None, confidence, source
    
    def _extract_korean_items(self, message: str) -> List[str]:
        """ë©”ì‹œì§€ì—ì„œ í•œê¸€ ì•„ì´í…œëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        korean_pattern = r'[ê°€-í£]+'
        korean_items = re.findall(korean_pattern, message)
        
        # ì˜ë¯¸ìˆëŠ” í•œê¸€ ë‹¨ì–´ë§Œ í•„í„°ë§ (1ê¸€ì ì œì™¸)
        meaningful_items = [item for item in korean_items if len(item) > 1]
        
        return list(set(meaningful_items))
    
    def _extract_item_names(self, message: str) -> List[str]:
        """ë©”ì‹œì§€ì—ì„œ ì•„ì´í…œëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        item_patterns = [
            r'(\w+)_(\w+)',
            r'([A-Z][a-z]+)([A-Z][a-z]+)',
            r'([ê°€-í£]+)',
        ]
        
        items = []
        for pattern in item_patterns:
            matches = re.findall(pattern, message)
            for match in matches:
                if isinstance(match, tuple):
                    items.extend(match)
                else:
                    items.append(match)
        
        return list(set(items))
    
    def _format_response_with_korean(self, response: str, translation_mapping: Dict[str, str]) -> str:
        """ì‘ë‹µì—ì„œ ì˜ì–´ ì•„ì´í…œëª…ì„ í•œê¸€(ì˜ì–´) í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not translation_mapping:
            return response
        
        formatted_response = response
        
        # ì˜ì–´ ì•„ì´í…œëª…ì„ í•œê¸€(ì˜ì–´) í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        for korean, english in translation_mapping.items():
            # ì˜ì–´ ì´ë¦„ì´ ì‘ë‹µì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if english in formatted_response:
                # ì²« ë²ˆì§¸ ë“±ì¥ë§Œ ë³€í™˜ (ì¤‘ë³µ ë°©ì§€)
                formatted_response = formatted_response.replace(
                    english, 
                    f"{korean}({english})", 
                    1
                )
        
        return formatted_response
    
    def generate_response(
        self, 
        message: str, 
        chat_history: List[Dict] = None, 
        modpack_name: str = "unknown",
        modpack_version: str = "1.0",
        user_uuid: str = None
    ) -> str:
        """í•˜ì´ë¸Œë¦¬ë“œ AI ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            if chat_history is None:
                chat_history = []
            
            # í•œê¸€ ì•„ì´í…œëª… ê°ì§€ ë° ë³€í™˜
            korean_items = self._extract_korean_items(message)
            translation_mapping = {}
            
            for korean_item in korean_items:
                english_name, confidence, source = self._translate_korean_to_english(
                    korean_item, modpack_name, user_uuid
                )
                if english_name and confidence > 0.5:
                    translation_mapping[korean_item] = english_name
            
            # ë³€í™˜ëœ ì˜ì–´ ì´ë¦„ìœ¼ë¡œ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
            processed_message = message
            for korean, english in translation_mapping.items():
                processed_message = processed_message.replace(korean, english)
            
            # 1. ë¡œì»¬ ì»¨í…ìŠ¤íŠ¸ (ë¹ ë¥¸ ì‘ë‹µ)
            local_context = self._get_local_context(processed_message, modpack_name, modpack_version, user_uuid)
            
            # 2. RAG ì»¨í…ìŠ¤íŠ¸ (í•„ìˆ˜ - ìƒì„¸ ì •ë³´)
            rag_context = self._get_rag_context(processed_message, modpack_name)
            
            # 3. ì›¹ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ (Gemini Proì—ì„œ í•­ìƒ í™œì„±í™”)
            web_context = ""
            current_model_info = self.available_models.get(self.current_model, {})
            web_search_enabled = current_model_info.get('web_search', False)
            
            if web_search_enabled:
                logger.info(f"ğŸŒ {self.current_model}ì—ì„œ ì›¹ê²€ìƒ‰ì„ í™œì„±í™”í•©ë‹ˆë‹¤.")
                web_context = self._get_web_search_context(processed_message, modpack_name)
            else:
                logger.info(f"ğŸ“– {self.current_model}ì—ì„œëŠ” ì›¹ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_prompt = self.system_prompt.format(
                modpack_name=modpack_name,
                modpack_version=modpack_version,
                context_info=local_context,
                rag_context=rag_context,
                web_search_context=web_context
            )
            
            # AI ì‘ë‹µ ìƒì„±
            provider = self.available_models[self.current_model]['provider']
            
            if provider == 'openai':
                ai_response = self._generate_openai_response(processed_message, chat_history, system_prompt)
            elif provider == 'anthropic':
                ai_response = self._generate_anthropic_response(processed_message, chat_history, system_prompt)
            elif provider == 'google':
                ai_response = self._generate_google_response(processed_message, chat_history, system_prompt)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ì œê³µì: {provider}")
            
            # ì‘ë‹µì—ì„œ ì˜ì–´ ì•„ì´í…œëª…ì„ í•œê¸€(ì˜ì–´) í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            formatted_response = self._format_response_with_korean(ai_response, translation_mapping)
            
            return formatted_response
                
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _generate_openai_response(self, message: str, chat_history: List[Dict], system_prompt: str) -> str:
        """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                messages = [{"role": "system", "content": system_prompt}]
                
                # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
                for msg in chat_history[-10:]:
                    if msg.get('user_message'):
                        messages.append({"role": "user", "content": msg['user_message']})
                    if msg.get('ai_response'):
                        messages.append({"role": "assistant", "content": msg['ai_response']})
                
                # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
                messages.append({"role": "user", "content": message})
                
                response = self.clients['openai'].chat.completions.create(
                    model=self.current_model,
                    messages=messages,
                    max_tokens=1000,
                    temperature=0.7,
                    timeout=30
                )
                
                return response.choices[0].message.content.strip()
                
            except openai.RateLimitError as e:
                logger.warning(f"OpenAI ë¬´ë£Œ í¬ë ˆë”§ ì†Œì§„ ë˜ëŠ” ì†ë„ ì œí•œ: {e}")
                return "âš ï¸ OpenAIì˜ ë¬´ë£Œ í¬ë ˆë”§ì´ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ AI ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”. (/modpackai models)"
            except openai.AuthenticationError as e:
                logger.error(f"OpenAI ì¸ì¦ ì˜¤ë¥˜: {e}")
                return "âš ï¸ OpenAI API í‚¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ AI ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
            except Exception as e:
                logger.warning(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    logger.error(f"OpenAI API ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {e}")
                    return "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _generate_anthropic_response(self, message: str, chat_history: List[Dict], system_prompt: str) -> str:
        """Anthropic APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            conversation = system_prompt + "\n\n"
            
            for msg in chat_history[-10:]:
                if msg.get('user_message'):
                    conversation += f"ì‚¬ìš©ì: {msg['user_message']}\n"
                if msg.get('ai_response'):
                    conversation += f"AI: {msg['ai_response']}\n"
            
            conversation += f"ì‚¬ìš©ì: {message}\nAI:"
            
            response = self.clients['anthropic'].messages.create(
                model=self.current_model,
                max_tokens=1000,
                temperature=0.7,
                messages=[{"role": "user", "content": conversation}]
            )
            
            return response.content[0].text.strip()
            
        except anthropic.RateLimitError as e:
            logger.warning(f"Anthropic ë¬´ë£Œ í¬ë ˆë”§ ì†Œì§„ ë˜ëŠ” ì†ë„ ì œí•œ: {e}")
            return "âš ï¸ Anthropicì˜ ë¬´ë£Œ í¬ë ˆë”§ì´ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ AI ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”. (/modpackai models)"
        except anthropic.AuthenticationError as e:
            logger.error(f"Anthropic ì¸ì¦ ì˜¤ë¥˜: {e}")
            return "âš ï¸ Anthropic API í‚¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ AI ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
        except Exception as e:
            logger.error(f"Anthropic API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _generate_google_response(self, message: str, chat_history: List[Dict], system_prompt: str) -> str:
        """Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            model = self.clients['google'].GenerativeModel('gemini-pro')
            
            # ëŒ€í™” ê¸°ë¡ êµ¬ì„±
            chat = model.start_chat(history=[])
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
            full_prompt = system_prompt + "\n\n"
            for msg in chat_history[-10:]:
                if msg.get('user_message'):
                    full_prompt += f"ì‚¬ìš©ì: {msg['user_message']}\n"
                if msg.get('ai_response'):
                    full_prompt += f"AI: {msg['ai_response']}\n"
            
            full_prompt += f"ì‚¬ìš©ì: {message}"
            
            response = chat.send_message(full_prompt)
            return response.text.strip()
            
        except Exception as e:
            error_msg = str(e).lower()
            if "quota" in error_msg or "limit" in error_msg:
                logger.warning(f"Google Gemini ë¬´ë£Œ í¬ë ˆë”§ ì†Œì§„: {e}")
                return "âš ï¸ Google Geminiì˜ ë¬´ë£Œ í¬ë ˆë”§ì´ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ AI ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”. (/modpackai models)"
            elif "authentication" in error_msg or "api_key" in error_msg:
                logger.error(f"Google Gemini ì¸ì¦ ì˜¤ë¥˜: {e}")
                return "âš ï¸ Google API í‚¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ AI ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
            else:
                logger.error(f"Google Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”." 